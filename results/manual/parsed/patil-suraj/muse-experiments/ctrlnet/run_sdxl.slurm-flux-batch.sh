#!/bin/bash
#FLUX: --job-name=sdxl
#FLUX: -N=2
#FLUX: -c=96
#FLUX: --exclusive
#FLUX: --queue=production-cluster
#FLUX: --priority=16

export MODEL_DIR='stabilityai/stable-diffusion-xl-base-1.0'
export LAUNCHER='accelerate launch \'
export CMD='$LAUNCHER $PROGRAM'
export NCCL_ASYNC_ERROR_HANDLING='1'
export NCCL_PROTO='simple'
export RDMAV_FORK_SAFE='1'
export FI_EFA_FORK_SAFE='1'
export FI_EFA_USE_DEVICE_RDMA='1'
export FI_PROVIDER='efa'
export FI_LOG_LEVEL='1'
export NCCL_IB_DISABLE='1'
export NCCL_SOCKET_IFNAME='ens'

set -x -e
source /admin/home/suraj/.bashrc
source /fsx/suraj/miniconda3/etc/profile.d/conda.sh
conda activate muse
echo "START TIME: $(date)"
REPO=/admin/home/suraj/code/muse-experiments/ctrlnet
OUTPUT_DIR=/fsx/suraj/sdxl
LOG_PATH=$OUTPUT_DIR/main_log.txt
ACCELERATE_CONFIG_FILE="$OUTPUT_DIR/${SLURM_JOB_ID}_accelerate_config.yaml.autogenerated"
mkdir -p $OUTPUT_DIR
touch $LOG_PATH
pushd $REPO
GPUS_PER_NODE=8
NNODES=$SLURM_NNODES
NUM_GPUS=$((GPUS_PER_NODE*SLURM_NNODES))
MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
MASTER_PORT=6000
cat << EOT > $ACCELERATE_CONFIG_FILE
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MULTI_GPU
fsdp_config: {}
machine_rank: 0
main_process_ip: $MASTER_ADDR
main_process_port: $MASTER_PORT
main_training_function: main
num_machines: $SLURM_NNODES
num_processes: $NUM_GPUS
use_cpu: false
EOT
export MODEL_DIR="stabilityai/stable-diffusion-xl-base-1.0"
PROGRAM="train_sdxl_txt2img.py \
    --pretrained_model_name_or_path=$MODEL_DIR \
    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \
    --output_dir=$OUTPUT_DIR \
    --mixed_precision=fp16 \
    --resolution=1024 \
    --learning_rate=3e-6 \
    --max_train_steps=50000 \
    --max_train_samples=504413 \
    --dataloader_num_workers=8 \
    --train_shards_path_or_url='pipe:aws s3 cp s3://muse-datasets/localised_narratives_open_images_wds/{000..101}.tar -' \
    --proportion_empty_prompts=0.05 \
    --validation_steps=500 \
    --train_batch_size=8 \
    --gradient_checkpointing \
    --enable_xformers_memory_efficient_attention \
    --gradient_accumulation_steps=1 \
    --seed=42 \
    --report_to=wandb \
    --resume_from_checkpoint=latest \
    --push_to_hub \
    --use_8bit_adam \
    --use_euler --use_ema \
    "
export LAUNCHER="accelerate launch \
    --rdzv_conf "rdzv_backend=c10d,rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT,max_restarts=0,tee=3" \
    --config_file $ACCELERATE_CONFIG_FILE \
    --main_process_ip $MASTER_ADDR \
    --main_process_port $MASTER_PORT \
    --num_processes $NUM_GPUS \
    --machine_rank \$SLURM_PROCID \
    "
export CMD="$LAUNCHER $PROGRAM"
echo $CMD
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_PROTO=simple
export RDMAV_FORK_SAFE=1
export FI_EFA_FORK_SAFE=1
export FI_EFA_USE_DEVICE_RDMA=1
export FI_PROVIDER=efa
export FI_LOG_LEVEL=1
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=ens
SRUN_ARGS=" \
    --wait=60 \
    --kill-on-bad-exit=1 \
    "
clear; srun $SRUN_ARGS --jobid $SLURM_JOB_ID bash -c "$CMD" 2>&1 | tee $LOG_PATH
echo "END TIME: $(date)"
